{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "For almost 200 years, we have been aware of that epidemics of diseases can be\n",
    "tracked and monitored indirectly. In London, around 1854, John Snow pioneered\n",
    "research into determining that [cholera outbreaks](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7150208/) could be traced to a particular\n",
    "source. Discoveries such as this led to not only a focus on [treating and\n",
    "cleaning](https://doi.org/10.2166/wst.2012.144) our water, but also to water being recognised as a valuable data source for disease\n",
    "monitoring. For example, there has been a lot of interest aimed towards\n",
    "inferring COVID-19 prevalence (i.e., number of positive cases in the general\n",
    "population) [from wastewater sources.](https://www.sciencedirect.com/science/article/pii/S2468584420300404)\n",
    "\n",
    "Using water-services to determine diseases has two primary benefits over\n",
    "targeted testing mechanisms:\n",
    "\n",
    "1. Data is relatively unbiased, by sick people being more likely to seek out tests\n",
    "   - There is also the possibility for reducing other biases such as socio-economic as everyone makes use of these services \n",
    "2. Population wide statistics can be determined whilst being cheaper and less\n",
    "   invasive than [alternative testing methods](https://www.sciencedirect.com/science/article/pii/S0043135420307181)\n",
    "\n",
    "Current (hopefully becoming past) world events have reinvigorated interest in\n",
    "using water to track diseases, specifically in using wastewater to estimate\n",
    "COVID-19 cases and outbreaks. For example, before 2020 interest in wastewater\n",
    "surveillance, as measured by Google trends (not the best metric, but one\n",
    "nonetheless) shows spikes in internet searches for the topic.\n",
    "\n",
    "![figures/g_trends.png](./figures/g_trends.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This trend is reflected in scientific publications as well, a quick search for\n",
    "[wastewater disease\n",
    "tracking](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&as_ylo=2018&q=wastewater+disease+tracking++-Covid+-SARS&btnG=)\n",
    "ignoring COVID as a keyword yields ~11,000 results since 2018. Whereas [not\n",
    "excluding these\n",
    "terms](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&as_ylo=2018&q=wastewater+disease+tracking+&btnG=)\n",
    "shows closer to ~15,000 results in the same time frame. Again, this is all just\n",
    "conjecture and more robust analysis would be needed to assert anything concrete\n",
    "about this trend. \n",
    "\n",
    "Regardless of the public and scientific interest in wastewater disease\n",
    "detection the key questions are: 1) can COVID-19 particle concentration be measured in wastewater? 2) Can the measurements be used to track COVID-19 prevalence?\n",
    "\n",
    "Well, that first question is already answered. Several countries have been\n",
    "activity using wastewater as a data collection method including:\n",
    "- The UK\n",
    "- Australia \n",
    "- Germany\n",
    "- Italy \n",
    "- Finland\n",
    "\n",
    "Even better, some countries, such as the UK, make these [data publicly\n",
    "available](https://post.parliament.uk/monitoring-wastewater-for-covid-19/). This\n",
    "enables the answering of the second question, \"is this data useful to monitor?\".\n",
    "\n",
    "Well, long story short, several papers have already been published and\n",
    "demonstrate the usefulness of wastewater COVID monitoring for public health:\n",
    "\n",
    "- [Controlling COVID-19 Pandemic through Wastewater Monitoring](https://www.scirp.org/journal/paperinformation.aspx?paperid=100434)\n",
    "- [Wastewater monitoring outperforms case numbers as a tool to track COVID-19 incidence dynamics when test positivity rates are high](https://www.sciencedirect.com/science/article/pii/S0043135421004504)\n",
    "- [Non-intrusive wastewater surveillance for monitoring of a residential building for COVID-19 cases](https://www.sciencedirect.com/science/article/pii/S0048969721024906)\n",
    "\n",
    "However, it's \n",
    "1. Educational\n",
    "2. More fun\n",
    "for us to explore the publicly available data ourselves. Hence, the purpose of\n",
    "this data story. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the data\n",
    "![https://zenodo.org/badge/DOI/10.5281/zenodo.7107061.svg](https://zenodo.org/badge/DOI/10.5281/zenodo.7107061.svg)\n",
    "\n",
    "Data is stored in Zenodo under [record7107061](https://zenodo.org/record/7107061). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://zenodo.org/record/7107061/files/data.zip\n",
    "#!unzip data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up Python environment and data\n",
    "\n",
    "First, we setup our data science workspace by organising which libraries we are\n",
    "using. Here we are using fairly standard Python3 libraries, all of which are\n",
    "available on [PyPi](https://pypi.org/)\n",
    "\n",
    "See running and installation [file](Cleaning_and_data_info.md) for complete setup guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'area'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/f4/m42_v8vj1fjc0wrwtb69xjt80000gr/T/ipykernel_6077/1659222584.py\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mglob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0marea\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marea\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdates\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmdates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_objects\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'area'"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import plotly.express as px\n",
    "from scipy.optimize import curve_fit, minimize\n",
    "import json \n",
    "from glob import glob\n",
    "from area import area\n",
    "import matplotlib.dates as mdates\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data \n",
    "\n",
    "We use pandas for all of our interactions with data tables, it exponentially\n",
    "makes our lives easier for working with tabular data. For example, the data is\n",
    "provided by the English government in an imperfect format for data processing,\n",
    "as such there are additional rows and entries of random notes and other data surrounding the\n",
    "elements we are interested in. Additionally, there are multiple \"sheets\" of\n",
    "data. Thus, when we load these data we skip a few rows and read only a specific\n",
    "sheet in the document. \n",
    "\n",
    "The data here is given as gene copies per litre (gc/l) for number of COVID-19\n",
    "RNA particles at a range of locations on various dates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_data = pd.read_excel(\"data/wastewater.ods\", sheet_name='Weekly_concentrations', skiprows=4)\n",
    "water_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these data we can see that there are 3 given identifiers for sampling\n",
    "location\n",
    "1. Region name \n",
    "2. Site code\n",
    "3. Site name\n",
    "\n",
    "Site code and name both have the same resolution, but site name is somewhat more\n",
    "friendly to read. [So really there are only 2\n",
    "identifiers.](https://en.wikipedia.org/wiki/Database_normalization)\n",
    "\n",
    "We can do some quick investigation on these data. Firstly we can count how many\n",
    "wastewater testing sites there are per region in England. This allows us to\n",
    "determine if there is a good distribution of these sites across geographic regions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_count = water_data.groupby(\"Region name\").size().reset_index().rename(columns={0:'Count'})\n",
    "fig = px.bar(region_count, x='Region name', y='Count')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we notice there are two outliers: \n",
    "\n",
    "1. London\n",
    "   - This one can be forgiven, it is a much smaller a region than the others \n",
    "2. The North East. \n",
    "   - This region has fewer sites than the other regions. This could also be\n",
    "    explained through population / area differences. \n",
    "   - This may be worth remembering and considering later in our analysis if we\n",
    "    notice issues with data.\n",
    "\n",
    "Next we can do a summary to count the total number of sites and regions. This\n",
    "will help us in determining how to direct our analysis and the scope at which we\n",
    "first want to focus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsites = len(water_data['Site name'].unique())\n",
    "print(f\"There are: {nsites} individual testing sites \\n\")\n",
    "\n",
    "print(\"Regions in England wastewater data:\")\n",
    "for idx, i in enumerate(water_data['Region name'].unique()):\n",
    "    print(f\"\\t{idx+1}: {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, looking at site name or code will give us 274 sources for data\n",
    "whereas for each region there are only 9 distinct groups to consider. Where each\n",
    "region has between 8 and 47 sites.\n",
    "\n",
    "For initial analysis we will look at this at the highest level — per region.\n",
    "Choosing to analyse at the region level will simplify our investigation. This\n",
    "will give a general insight into the relationship between wastewater COVID RNA\n",
    "levels and case numbers. \n",
    "\n",
    "It is entirely possible, probable even, that each region will have variation\n",
    "within them. For example cities compared to more rural regions could have\n",
    "significant differences. However, for the sake of a broad overview we will use\n",
    "regions as a starting point. Additionally, if we were to observe few differences between regions in terms of\n",
    "wastewater COVID-19 RNA samples then we could contemplate considering England as\n",
    "a single region and simplify analysis even further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Averaging data for each region \n",
    "\n",
    "For each of these regions we need to average the COVID-19 RNA samples per litre of water,\n",
    "but also take the sum of the population all with respect to the date. Currently\n",
    "the data is shaped such that each sampling date is given as a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `agg` function and a\n",
    "`groupby` we can find the mean gene count per litre (gc/l) for each date. The\n",
    "`agg` method also allows us to specify that for population we require a sum for\n",
    "the regions' sites, not\n",
    "an average. It is important to note that population is a dynamic variable,\n",
    "however in these data the value does not change with respect to time. \n",
    "\n",
    "To save some long typing out of column names we can construct an aggregate\n",
    "dictionary which applies a mean function to all the columns with a date and sum\n",
    "function to the population. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aggs = {k:'mean' for k in water_data.columns[4:]}\n",
    "data_aggs['Population'] = 'sum'\n",
    "water_data_regions = water_data.groupby('Region name').agg(data_aggs).reset_index()\n",
    "water_data_regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this is real data it is fully expected that there will be some invalid or\n",
    "missing data. We can check for `NaN` values in our dataframe using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_data_regions[water_data_regions.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two rows containing `NaN` values is not a huge issue. It's good to be aware of\n",
    "as it could cause errors later on in our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading spatial data \n",
    "\n",
    "Personally, I love tables of data, but sometimes visually viewing things can\n",
    "really help to rationalise and compare results. For interactive plots which can\n",
    "be adjusted without editing the underlying code I like to use Plotly.\n",
    "\n",
    "Plotly allows for very pretty and very fast plotting of geography data through\n",
    "the use of the [GEOJSON file format](https://geojson.org/). We use the json library and python file reader to load the\n",
    "data file which contains a series of polygons which makeup UK region borders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/uk_regions.geojson\") as f:\n",
    "    UK_geojson = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting this dataobject we can see it contains multidimensional data. In\n",
    "`features` we see that there are entries with parameters for each region.\n",
    "Specifically, we see that one particular entry contains data similar to our\n",
    "Region name which we have in the wastewater data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UK_geojson['features'][0]['properties']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching spatial data with waste water data \n",
    "\n",
    "The feature named `eer17nm` matches with \"Region\n",
    "name\" in the wastewater table. With one minor exception, the name for East of England\n",
    "is slightly different, so we can rename it in the wastewater dataframe to make\n",
    "plotting and analysis easier moving forward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_data_regions.loc[water_data_regions['Region name'] == 'East of England', 'Region name'] = 'Eastern'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test if this match-up is successful, and if our data is sensible we make a\n",
    "simple Choropleth plot using population as our information values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap_range = (water_data_regions['Population'].min(), water_data_regions['Population'].max())\n",
    "fig = px.choropleth(water_data_regions, geojson=UK_geojson, locations='Region name', color='Population', featureidkey=\"properties.eer17nm\", range_color=cmap_range, )\n",
    "fig.update_geos(fitbounds=\"locations\", visible=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing region features\n",
    "\n",
    "A major red-flag to this analysis is the population of London. The previous plot\n",
    "shows that for most regions the population between them reasonably similar,\n",
    "however London's population is approximately 2x greater than any other\n",
    "individual region. Therefore, we should think carefully about how we include\n",
    "this in our analysis. It might be that the best course of action would be to\n",
    "analyse London independently or perhaps include some additional parameter in any\n",
    "models which include a population statistic for each region.\n",
    "\n",
    "Permit me to interject with a bit of personal data science philosophy, I like to\n",
    "think that the most exciting things happen when data stands out or doesn't\n",
    "appear to make sense. Explaining these standouts gives additional purpose to our\n",
    "analysis. As we go through this process we should keep a collection\n",
    "of reasons for possible standouts and use them to form hypotheses about what is\n",
    "going on with the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Population density \n",
    "\n",
    "From these data it is clear that there is one outlining area with a high\n",
    "population but small area. However, this has been quantified only visually for\n",
    "the area. Therefore we should try and put some numbers to this so that we have a\n",
    "better idea how significant this density difference really is. Using these and\n",
    "another library, `area`, we can approximate region area of objects in a GEOJSON\n",
    "file. \n",
    "\n",
    "In order to get the area of a region we need to grab the specific polygon for\n",
    "which it belongs. Here we can write a quick function to perform this lookup.\n",
    "Computationally it is very inefficient as we have to loop through all polygons\n",
    "in the GEOJSON file and also perform a comparison between region names to\n",
    "evaluate if it is the correct one. However, computational effort is cheap and\n",
    "our time is not, so we use it as is.\n",
    "\n",
    "With this we perform an apply function in pandas to create a new column in our\n",
    "data table. From this we calculate a Population Per Area (PPA). In the apply\n",
    "function we make use of a `lambda` function, this can be thought of as a small,\n",
    "single expression function which returns the result of said expression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is incredibly inefficient, but computational power is cheap and developer\n",
    "# time is not! \n",
    "\n",
    "def lookup_district_area(district):\n",
    "    \"Gets district area in squared meters\"\n",
    "    for poly in UK_geojson['features']:\n",
    "        if poly['properties']['eer17nm'] == district:\n",
    "            return area(poly['geometry'])\n",
    "    return -1 # Something broke\n",
    "\n",
    "water_data_regions['Area'] = water_data_regions.apply(lambda x: lookup_district_area(x['Region name']), axis=1)\n",
    "\n",
    "water_data_regions['PPA'] = water_data_regions['Population']/water_data_regions['Area']\n",
    "\n",
    "water_data_regions[['Region name','Area', 'Population', 'PPA']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, and unsurprisingly the PPA of the London region is orders of\n",
    "magnitude greater than any other regions of England"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(water_data_regions, x='Region name', y='PPA')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these population per area (PPA) results a very strong case could be made to\n",
    "consider the London region as a separate and distinct area when performing\n",
    "further analysis. Particularly when disease dynamics are different in highly\n",
    "populated areas this could cause issues when making comparisons with London and\n",
    "the other regions of England <sup>[citation needed]</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID-19 gene counts per litre over time in England's regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For epidemics we are often more interested in how the disease is changing over\n",
    "time. An individual time point cannot tell us if the disease is becoming more or\n",
    "less prevalent. Therefore we next want to organise our data in such a way that\n",
    "we can compare gc/l and how they change in each region overtime. \n",
    "\n",
    "Currently, the data is in a format where columns exist for each date entry. Our\n",
    "target is to have a format where `Date` is a single column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_data_regions.columns[4:-3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data available as of August 2022 we have 29 entries which run from\n",
    "22/06/2021 - 04/01/2022. \n",
    "\n",
    "Again, I'd like to\n",
    "take this opportunity to express my love for pandas as this form of data\n",
    "wrangling is completed in a single beautiful line of code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_data_dates = water_data_regions.melt(id_vars=['Region name','Population','Area','PPA'], var_name='Date', value_name='gc/l')\n",
    "water_data_dates['Date'] = pd.to_datetime(water_data_dates['Date'], dayfirst=True)\n",
    "water_data_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we can ask the question: how large is the variation of gc/l\n",
    "measurements between England's regions? \n",
    "\n",
    "To answer this, and to decide if we can take a country-scale look at the data or\n",
    "should stick with region scale, we can perform another aggregation of the data\n",
    "to get the mean behaviour of England's wastewater values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_data_dates_mean = water_data_dates.groupby('Date').agg( {'gc/l':'mean'} ).reset_index().sort_values(by='Date')\n",
    "water_data_dates_mean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is the mean a good approximation for gc/l or does each region vary greatly?\n",
    "\n",
    "We then plot these averaged data against the region specific data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(water_data_dates, x='Date', y='gc/l', color='Region name')\n",
    "fig.update_traces(opacity=0.4, line_width=1)\n",
    "water_data_dates_mean['Region name'] = 'mean' # adding this column for label in legend\n",
    "fig.add_trace(px.line(water_data_dates_mean, x='Date', y='gc/l', color='Region name').data[0])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually, the mean values for gc/l appear to follow a general trend over time\n",
    "for most of the regions, there are some notable disagreements around August 2021\n",
    "and January 2022. Therefore, we next will investigate and quantify how large\n",
    "these differences are between regions and England's mean gc/l for each date. \n",
    "What about the percentage difference from the mean?\n",
    "\n",
    "\n",
    "To match across the two dataframes we make a lookup dictionary for the mean\n",
    "values. We also define a lambda function `pc_diff_calc` which calculates the\n",
    "percentage difference between values `a` and `b`. This gives a simple metric by\n",
    "which we can see if any particular dates and or regions greatly differ from mean\n",
    "gc/l estimates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_per_dates = {water_data_dates_mean['Date'][i]:water_data_dates_mean['gc/l'][i] for i in range(len(water_data_dates_mean))}\n",
    "pc_diff_calc = lambda a,b: abs((a - b) / b) * 100.0\n",
    "water_data_dates['Difference to mean'] = water_data_dates.apply(lambda x: pc_diff_calc(x['gc/l'], mean_per_dates[x['Date']] ), axis=1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there was a strong agreement between the mean estimate and the results in\n",
    "each region then we would expect the average of the values to be close to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_differences = water_data_dates.groupby('Region name').mean()['Difference to mean'].reset_index().sort_values(by='Difference to mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(mean_differences, x='Region name', y='Difference to mean')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curiously there seems to be a gradient of results here. With no single region\n",
    "standing out. Even our predicted outlier of London is comparable with another\n",
    "region in terms of it's  disagreement to the average gc/l.\n",
    "\n",
    "To explore more closely where these differences arise, and when, we plot the\n",
    "same data over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(water_data_dates, x='Date', y='Difference to mean', color='Region name')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From plotting these data we can see that whilst initially the general trend\n",
    "seemed similar between each date and region with the mean estimate. However,\n",
    "there are a number of extreme values which would suggest that averaging at a\n",
    "country level is not going to yield true insights into gc/l trends in England\n",
    "overtime. \n",
    "\n",
    "One additional check we can do is quantify and view the sum of these\n",
    "disagreements. This would show us two things. Firstly, if any regions are close\n",
    "to matching the mean or if they are, either consistently or from outliers,\n",
    "largely different from the mean estimate. Secondly, it can allow us to make a\n",
    "rough comparison between the regions themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an interactive spatial-temporal map for gc/l data\n",
    "\n",
    "As previous discussed there are some missing data, data also have some\n",
    "inconsistencies in that they are not all reported on the same day. We resolve\n",
    "this by grouping data to the nearest month and dropping the day component (this\n",
    "also will simplify downstream analysis when we join with additional data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_data_pm = water_data_dates.groupby([pd.Grouper(key='Date', freq='M'), 'Region name'])['gc/l'].mean().reset_index()\n",
    "water_data_pm['Date'] = water_data_pm['Date'].dt.strftime('%m/%Y')\n",
    "water_data_pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With plotly express we can create a chropleth plot of England and add a date\n",
    "slider to view the change in gc/l over time. These plots are showing the same\n",
    "data we just analysed, but in a more visually attractive style!\n",
    "\n",
    "Re-plotting the same data in a different style can be useful, for example here\n",
    "it makes the increase of gc/l seen in July and August 2021 stand out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap_range = (water_data_pm['gc/l'].min(), water_data_pm['gc/l'].max())\n",
    "fig = px.choropleth(water_data_pm, geojson=UK_geojson, locations='Region name', color='gc/l', featureidkey=\"properties.eer17nm\", animation_frame=\"Date\", range_color=cmap_range)\n",
    "fig.update_geos(fitbounds=\"locations\", visible=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incorporating COVID-19 reported cases data\n",
    "\n",
    "Now, just a reminder, what we are truly interested in is whether or not we can\n",
    "use these data to estimate COVID-19 cases. Therefore, we still have a major\n",
    "component of data missing - case numbers. Once again, the UK government makes\n",
    "these data open and freely available. \n",
    "\n",
    "We wrangle these data with some pre-processing steps to make it easier to work\n",
    "with. Like our previous analysis there is a discrepancy in the naming of East of\n",
    "England, thus we can fix that here. \n",
    "\n",
    "Whilst we are tweaking the data we perform another rounding of dates. This way\n",
    "we have case numbers aligned to weeks, we convert this to nearest month and therefore can be directly tied into\n",
    "our wastewater data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_cases_by_region = pd.read_csv(\"./data/covid_cases/covid_data_combined.csv\")\n",
    "covid_cases_by_region.loc[covid_cases_by_region['areaName'] == 'East of England', 'areaName'] = 'Eastern'\n",
    "covid_cases_by_region['date'] = pd.to_datetime(covid_cases_by_region['date']).dt.strftime('%m/%Y')\n",
    "covid_cases_by_region = covid_cases_by_region.groupby(['areaName','date']).sum().reset_index().drop(['Unnamed: 0'], axis=1)\n",
    "covid_cases_by_region.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To link these data we use the merge function of pandas. We create a new\n",
    "dataframe merging on the date and region name values of both tales of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_water_pm = pd.merge(water_data_pm, covid_cases_by_region, right_on=['date', 'areaName'], left_on=['Date', 'Region name'])\n",
    "cases_water_pm.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without looking at the data it would be a sensible hypothesis to state that gc/l\n",
    "of COVID-19 RNAs in wastewater is somewhat proportional to the number of positive\n",
    "cases detected. We can quick and visually explore this idea with a dual axes\n",
    "plot of the two values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "# Add traces\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=cases_water_pm['Date'], y=cases_water_pm['gc/l'], name=\"gc/l\"),\n",
    "    secondary_y=False,\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=cases_water_pm['Date'], y=cases_water_pm['newCasesBySpecimenDate'], name=\"Cases by date\"),\n",
    "    secondary_y=True)\n",
    "\n",
    "# Add figure title\n",
    "fig.update_layout(\n",
    "    title_text=\"gc/l vs. cases\"\n",
    ")\n",
    "\n",
    "# Set x-axis title\n",
    "fig.update_xaxes(title_text=\"Date\")\n",
    "\n",
    "# Set y-axes titles\n",
    "fig.update_yaxes(title_text=\"gc/l\", secondary_y=False)\n",
    "fig.update_yaxes(title_text=\"Cases by specimen date\", secondary_y=True)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first look isn't convincing. General trends of up/down at the same time\n",
    "seem to be correct, but the magnitude is poorly reflected. For example the cases\n",
    "in Summer 2021 are not so different with Autumn, but there's a significant\n",
    "upsurge of gc/l. Data agree much better in winter also. \n",
    "\n",
    "Important to note here is that we are considering all regions lumped together.\n",
    "Hence the error bars shown in this plot. Our next step would be to consider this\n",
    "in the per-region scope. It would be interesting to consider if some regions'\n",
    "case numbers are\n",
    "more accurately represented by wastewater gc/l than others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs=[ [ {\"secondary_y\": True} for _ in range(3)] for _ in range(3)  ]\n",
    "fig = make_subplots(rows=3, cols=3,  subplot_titles=cases_water_pm['Region name'].unique())\n",
    "\n",
    "#normalise a bit for plotting\n",
    "cases_water_pm['newCasesBySpecimenDateNorm'] = cases_water_pm['newCasesBySpecimenDate'] / cases_water_pm['newCasesBySpecimenDate'].max()\n",
    "cases_water_pm['gc/lNorm'] = cases_water_pm['gc/l'] / cases_water_pm['gc/l'].max()\n",
    "\n",
    "leg = True\n",
    "make_trace_cases = lambda rn, leg: go.Scatter(x=cases_water_pm[cases_water_pm['Region name'] == rn ]['Date'], y=cases_water_pm[cases_water_pm['Region name'] == rn ]['newCasesBySpecimenDateNorm'], name=\"Cases by date\", marker = {'color' : 'red'}, showlegend=leg,)\n",
    "make_trace_gcl = lambda rn, leg: go.Scatter(x=cases_water_pm[cases_water_pm['Region name'] == rn ]['Date'], y=cases_water_pm[cases_water_pm['Region name'] == rn ]['gc/lNorm'], name=\"gc/l\", marker = {'color' : 'blue'}, showlegend=leg)\n",
    "\n",
    "for idx in range(3):\n",
    "    for idy in range(3):\n",
    "        i = idx*3 + idy\n",
    "        rn = cases_water_pm['Region name'].unique()[i]\n",
    "        t1 = make_trace_cases(rn, leg)\n",
    "        t2 = make_trace_gcl(rn, leg)\n",
    "        fig.append_trace(t1, idx+1, idy+1)\n",
    "        fig.append_trace(t2, idx+1, idy+1)\n",
    "\n",
    "\n",
    "fig['layout'].update(autosize= False, \n",
    "              width= 2*800, \n",
    "              height= 800, \n",
    "              showlegend=leg,\n",
    "              hovermode='x')\n",
    "\n",
    "fig.update_xaxes(tickangle=45)\n",
    "\n",
    "names = set()\n",
    "fig.for_each_trace(\n",
    "    lambda trace:\n",
    "        trace.update(showlegend=False)\n",
    "        if (trace.name in names) else names.add(trace.name))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, most of these regions have a vaguely similar trend of both gc/l\n",
    "and case numbers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can we use the gc/l data to predict covid cases? \n",
    "\n",
    "Let's chose a specific area to use as a 'case study' and to focus our analysis. Having spent the past 4 years in East Anglia I am biased to choose the 'Eastern'\n",
    "region for an initial test. So, if we kept up with case reporting during the COVID-19 pandemic then we know that\n",
    "case numbers typically follow an exponential curve which is normally given as \n",
    "\n",
    "$$ y = m^{bx} $$\n",
    "\n",
    "where $y$ is the output variable, $m,b$ are coefficients and $x$ as a\n",
    "dependant variable. The coefficient are set by using a curve fit function which\n",
    "finds the optimal values for $m,b$ which when used with $x$ give the most\n",
    "accurate estimation of $y$ (which is possible with the given function). \n",
    "These values for $m,b$ are stored in `popt`. \n",
    "\n",
    "We can show that this kind of model is a relatively good\n",
    "approximation for COVID-19 cases over time by fitting this function to the data\n",
    "we have available. \n",
    "\n",
    "A few key things to note: \n",
    "1. We convert our dates to a `datenum` with the mdates class of matplotlib - this gives them a floating point representation.\n",
    "2. $x$ will be this `datenum` value and our $y$ is COVID-19 cases.\n",
    "3. $\\hat{y}$ is our approximation of case numbers for a given date.\n",
    "4. These models have the clear limitation of not being limited. That is to say that they have no cap and eventually become wrong. \n",
    "   - e.g. Dates next year are likely predicting more cases than humans currently alive. \n",
    "5. We use `start` as a variable to make curve fitting more accurate.\n",
    "   - The curve fitting function is blind\n",
    "   to what the optimal values should be and has a search space of -inf to +inf,\n",
    "   thus we can speed up the process by specifying an approximate value to begin\n",
    "   with in the `start` variable. \n",
    "\n",
    "\n",
    "To evaluate if we can predict prior case numbers as well as future we have\n",
    "looked a month ahead and behind but only fit the data from June to December 2021.\n",
    "Please do note and understand that if we used a different date range this\n",
    "analysis would very different. I intentionally use this range to avoid the\n",
    "effects of the UK's implementation of the so-called [Plan\n",
    "B](https://www.gov.uk/government/news/prime-minister-confirms-move-to-plan-b-in-england)\n",
    "to halt infection spread. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def exp_f(x, m, b):\n",
    "    return m*np.exp(b*x)\n",
    "\n",
    "# One solution to working with dates is to consider them numerically \n",
    "# This may not be an ideal way of working with dates but it works for these simple\n",
    "# analysis \n",
    "\n",
    "cases_water_pm['DateNum'] = mdates.date2num(pd.to_datetime(cases_water_pm['Date']))\n",
    "eastern_cases_water = cases_water_pm[cases_water_pm['Region name'] == 'Eastern' ]\n",
    "x1 = eastern_cases_water['DateNum'].values[1:-1]\n",
    "y1 = eastern_cases_water['newCasesBySpecimenDate'][1:-1]\n",
    "start = 1e-3\n",
    "\n",
    "# optimal values for parameters and their estimated covarience\n",
    "# We're really only interested in popt for now.\n",
    "popt, pcov = curve_fit(exp_f, x1, y1, p0=[start,start], maxfev=50000)\n",
    "\n",
    "covid_cases_by_region['date'] = pd.to_datetime(covid_cases_by_region['date'])\n",
    "covid_cases_by_region = covid_cases_by_region[(covid_cases_by_region['date'] > '2021-05') & (covid_cases_by_region['date'] <= '2022-01') ]\n",
    "covid_cases_by_region = covid_cases_by_region[covid_cases_by_region['areaName'] == 'Eastern'].sort_values(by='date')\n",
    "\n",
    "X_nu = mdates.date2num(covid_cases_by_region['date'])\n",
    "y_nu = covid_cases_by_region['newCasesBySpecimenDate']\n",
    "X_approx = np.linspace(X_nu.min(), X_nu.max(), num=1000)\n",
    "\n",
    "y_hat = exp_f(X_approx, *popt)\n",
    "\n",
    "\n",
    "fig = go.Figure(data=go.Scatter(x=mdates.num2date(X_nu), y=y_nu, name='Number of new positive cases'))\n",
    "fig.add_trace(go.Scatter(x=mdates.num2date(X_approx), y=y_hat, name='Exponential Curve Fit'))\n",
    "fig.add_trace(go.Scatter(x=mdates.num2date(x1), y=y1, name='Training values'))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like a very good fit, although it's more useful if we can get a\n",
    "numerical quantification of this fit. A simple method for this is to calculate\n",
    "an $R^2$ value. The $R^2$ is essentially a \"goodness-of-fit\" measure, it looks\n",
    "at the variance explained by the model with respect to the total variance. Thus,\n",
    "values for $R^2$ close to 1 are considered better than those close to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_r2(X, popt, y1, f):\n",
    "    y_fit = f(X, *popt)\n",
    "    ss_res = np.sum((y1 - y_fit) ** 2)\n",
    "    ss_tot = np.sum((y1 - np.mean(y1)) ** 2)\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    return r2\n",
    "\n",
    "r2_exp_cases = calc_r2(X_nu, popt, y_nu, exp_f)\n",
    "print(f'R^2: {r2_exp_cases}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is somewhat a loaded question given that we have already made some\n",
    "observations about the data, but we next could ask \"does wastewater gc/l follow\n",
    "a similar curve\". The logic being that gc/l in wastewater must be proportional\n",
    "to case numbers and therefore would have a similar exponential trend. \n",
    "\n",
    "To do this we first estimate the relationship between gc/l and case numbers, we\n",
    "then can re-use the initial model to using predictions from the gc/l -> case\n",
    "numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = eastern_cases_water['gc/l'][1:-1]\n",
    "popt, pcov = curve_fit(exp_f, x1, y2, p0=[start,start], maxfev=50000)\n",
    "y3 = eastern_cases_water['gc/l']\n",
    "x3 = eastern_cases_water['DateNum']\n",
    "y_hat = exp_f(X_approx, *popt)\n",
    "\n",
    "fig = go.Figure(data=go.Scatter(x=mdates.num2date(x3), y=y3, name='gc/l'))\n",
    "fig.add_trace(go.Scatter(x=mdates.num2date(X_approx), y=y_hat, name='Exponential Curve Fit'))\n",
    "fig.add_trace(go.Scatter(x=mdates.num2date(x1), y=y2, name='Training values'))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_exp_gcl = calc_r2(x3, popt, y3, exp_f)\n",
    "print(f'R^2: {r2_exp_gcl}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Completely unsurprisingly we can see that these data are not so easily modelled\n",
    "as case numbers are. The negative value of R^2 indicates that this is a model so\n",
    "bad that a horizontal line $y=x+m$ would be a better estimator! \n",
    "Therefore it is clear that if a relationship exists between gc/l and case numbers\n",
    "then it is likely a more complex relationship.\n",
    "\n",
    "Nonetheless, we can still attempt to use wastewater gc/l values to build an\n",
    "estimator in a similar fashion as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popt, pcov = curve_fit(exp_f, y2, y1, p0=[start,start], maxfev=50000)\n",
    "y1_est = exp_f(y2, *popt)\n",
    "\n",
    "popt, pcov = curve_fit(exp_f, x1, y1_est, p0=[start,start], maxfev=50000)\n",
    "y_hat = exp_f(X_approx, *popt)\n",
    "\n",
    "fig = go.Figure(data=go.Scatter(x=mdates.num2date(X_nu), y=y_nu, name='Number of new positive cases'))\n",
    "fig.add_trace(go.Scatter(x=mdates.num2date(X_approx), y=y_hat, name='Exponential Curve Fit'))\n",
    "fig.add_trace(go.Scatter(x=mdates.num2date(x1), y=y1, name='Training values'))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_exp_gcl_to_cn = calc_r2(X_nu, popt, y_nu, exp_f)\n",
    "print(f'R^2: {r2_exp_gcl_to_cn}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can next ask \"can date and gc/l be used to increase\n",
    "accuracy of this model?\". \n",
    "\n",
    "We use a similar function as before, but modify it slightly to accept more\n",
    "determining variables in the form of $x_i$\n",
    "\n",
    "$$y = m^{bx_1} + x_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_f2(X, m, b, c):\n",
    "    return m*np.exp(b*X[0]) * X[1]*c\n",
    "\n",
    "X = np.vstack((x1,y2))\n",
    "popt, pcov = curve_fit(exp_f2, X, y1, p0=[start,start,start], maxfev=50000)\n",
    "\n",
    "X_approx = np.dstack((np.linspace(X_nu.min(), X_nu.max(), num=1000), np.linspace(X[1].min(), X[1].max(), num=1000)))[0].T\n",
    "y_hat = exp_f2(X_approx, *popt)\n",
    "\n",
    "fig = go.Figure(data=go.Scatter(x=mdates.num2date(X_nu), y=y_nu, name='Number of new positive cases'))\n",
    "fig.add_trace(go.Scatter(x=mdates.num2date(X_approx[0]), y=y_hat, name='Exponential Curve Fit'))\n",
    "fig.add_trace(go.Scatter(x=mdates.num2date(x1), y=y1, name='Training values'))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2 = np.vstack((X_nu, x3))\n",
    "r2_exp_cases_gcl = calc_r2(X_2, popt, y_nu, exp_f2)\n",
    "print(f'R^2: {r2_exp_cases_gcl}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"R^2 value for model predicting cases with exponential function: {r2_exp_cases}\")\n",
    "print(f\"R^2 value for model predicting gc/l with exponential function: {r2_exp_gcl}\")\n",
    "print(f\"R^2 value for model predicting cases using case # and gc/l with exponential function: {r2_exp_cases_gcl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly incorporating the wastewater data does not harm the predictive\n",
    "power of this exponential model. It doesn't make it *much* better, but it doesn't make\n",
    "it worse. Well, one could argue that simple models i.e., fewer parameters, are\n",
    "better overall. \n",
    "\n",
    "From here we can think about reasons why this wastewater data is not correlating\n",
    "well with COVID-19 test cases. Blaming testing bias, vaccination rates or some\n",
    "other factor could be a viable answer. Another could be the effect of weather,\n",
    "specifically rainfall. Rainfall in England is highly variable over the course of\n",
    "the year and as such could result in the concentration of COVID-19 RNA particles\n",
    "to be higher or lower dependent on rainfall. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather data \n",
    "\n",
    "Like all the data used thus far weather data in the UK is also relatively\n",
    "easy to come by and is in a semi-accessible format. The weather data is extensive and spans more than a century of monthly\n",
    "data. The data we are loading in here has been pre-processed according to the\n",
    "`Cleaning_and_data_info.md` file. \n",
    "\n",
    "Firstly, we load in the data and can remove data prior to June 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rainfall_files = glob(\"./data/rainfall/*.csv\")\n",
    "rainfall_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As each region has been given its own datafile we save the filenames into a\n",
    "dictionary. We can then use this dictionary to hold each individual file until we\n",
    "match it up with the data we've been using previously for wastewater/COVID-19 cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rainfall_dfs = {}\n",
    "for rff in rainfall_files:\n",
    "    rff_name = rff.split('/')[-1].split('.')[0]\n",
    "    rainfall_dfs[rff_name] = pd.read_csv(rff)[-4:]\n",
    "rainfall_dfs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching rainfall to wastewater locations\n",
    "\n",
    "Unfortunately the names and locations used for the rainfall measurements are not\n",
    "ideal, thus the quickest and easiest solution is to manually create a dictionary\n",
    "to look up and match the data \n",
    "\n",
    "Below we have two maps, the left showing how the weather data are labelled by the\n",
    "UK MET office, the right on how our case data and wastewater data are roughly\n",
    "categorised. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Met office regions| Wastewater regions|\n",
    "|---|---|\n",
    "| ![Met](https://www.metoffice.gov.uk/binaries/content/gallery/metofficegovuk/images/weather/learn-about/past-uk-weather-events/uk-regional-boundaries/districtregions.gif/districtregions.gif/metofficegovuk%3Axsmall) | ![link](http://projectbritain.com/regions/images/regions.png) |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is crucial to note that these are very rough approximations based on visually\n",
    "assigning a close matching region. It is likely that a more robust\n",
    "process is possible and that more local measurements for rainfall is available\n",
    "(but a project in and of itself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are rough approximations to the regions\n",
    "region_to_rainfall_location = {\n",
    "'North West':'England_NW_and_N_Wales',\n",
    "'South East' : 'England_SE_and_Central_S',\n",
    "'South West' : 'England_SW_and_S_Wales',\n",
    " 'London':'England_SE_and_Central_S',\n",
    " 'West Midlands' : 'Midlands', \n",
    " 'Yorkshire and The Humber': 'England_E_and_NE',\n",
    "   'North East':'England_E_and_NE',\n",
    "    'East Midlands':'Midlands',\n",
    "      'Eastern':'East_Anglia'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rainfall_dfs[rff_name][rainfall_dfs[rff_name].columns[:-5]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape and format of these rainfall data need altered to work with our\n",
    "current setup. Similar to before when we had date column headings we now have\n",
    "each row representing an individual year. This time we perform a melt function\n",
    "to convert these columns into a single column. We combine this column with the\n",
    "existing year column to build a year/month string which can be formatted by\n",
    "pandas to %m/%y format like the rest of our data.\n",
    "\n",
    "We repeat this process for each region, create a region ID column and\n",
    "concatenate all of the dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_rain_pm_lst = []\n",
    "for rff_name in rainfall_dfs.keys():\n",
    "    rf_melted = rainfall_dfs[rff_name][rainfall_dfs[rff_name].columns[:-5]].melt(id_vars='year', value_name='rainfall')\n",
    "    rf_melted['Date'] = pd.to_datetime(rf_melted.apply(lambda x: f\"{x['year']}/{x['variable']}\", axis=1)).dt.strftime('%m/%Y')\n",
    "    rf_melted['Location'] = rff_name\n",
    "    rf_rain_pm_lst.append(rf_melted)\n",
    "rf_rain_pm = pd.concat(rf_rain_pm_lst)\n",
    "rf_rain_pm = rf_rain_pm.drop(['variable','year'], axis=1)\n",
    "rf_rain_pm.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data is in a workable format for us we can go about adding it to our\n",
    "existing dataframe. We use the dictionary created above for matching regions\n",
    "between rainfall data and the wastewater to match these up. From this we can use\n",
    "a lookup function to get the rainfall on a particular date in a specific region. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_water_pm['rainfall_region'] = cases_water_pm.apply(lambda x: region_to_rainfall_location[x['Region name']], axis=1)\n",
    "\n",
    "def lookup_date_region_rain(date, region):\n",
    "    row = rf_rain_pm.loc[(rf_rain_pm[\"Date\"]==date)&( rf_rain_pm[\"Location\"]==region)]['rainfall']\n",
    "    if len(row) == 0:\n",
    "        return 'n/a'\n",
    "    return row.values[0]\n",
    "\n",
    "lookup_rf = lambda x: lookup_date_region_rain(x['Date'], x['rainfall_region'])\n",
    "cases_water_pm['Rainfall'] = cases_water_pm.apply(lookup_rf, axis=1)\n",
    "cases_water_pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we attempt to use this data in our models we can first have a look at the\n",
    "general trend for these variables: \n",
    "1. Wastewater gc/l\n",
    "2. Rainfall (mm)\n",
    "3. COVID-19 positive case numbers\n",
    "\n",
    "Keeping it simple to start off with we will average and group by date, thereby\n",
    "ignoring the spatial component of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_vs_gcl = cases_water_pm.groupby(['Date'])[['gc/l', 'Rainfall', 'newCasesBySpecimenDate']].mean().reset_index()\n",
    "rain_vs_gcl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These three columns are widely different and do not follow the same scale. To\n",
    "plot them together and make a visual observation of trends over time we perform\n",
    "a simple normalisation. Each column is divided by its maximum value which\n",
    "results in values for each $\\in [0,1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_vs_gcl['gc/lNorm'] = rain_vs_gcl['gc/l']/ rain_vs_gcl['gc/l'].max()\n",
    "rain_vs_gcl['rainfallNorm'] = rain_vs_gcl['Rainfall']/ rain_vs_gcl['Rainfall'].max()\n",
    "rain_vs_gcl['newCasesBySpecimenDateNorm'] = rain_vs_gcl['newCasesBySpecimenDate']/ rain_vs_gcl['newCasesBySpecimenDate'].max()\n",
    "\n",
    "rain_vs_gcl_melted = rain_vs_gcl[['Date', 'gc/lNorm', 'rainfallNorm', 'newCasesBySpecimenDateNorm']].melt(id_vars='Date')\n",
    "\n",
    "fig = px.line(rain_vs_gcl_melted, x='Date', y='value', color='variable')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correcting wastewater values using rainfall data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reason out that wastewater data for COVID-19 gc/l is somehow affected by\n",
    "the rain. If we hypothesise that this is a linear relationship i.e., gc/l is in\n",
    "someway proportional to rain amount then it may be possible that gc/l values, if\n",
    "scaled correctly, could help give more accurate case number predictions!\n",
    "\n",
    "With this in mind our model becomes: \n",
    "\n",
    "$$y = a^{bx_d} cx_wx_r$$\n",
    "\n",
    "where $x_d$ is date, $x_w$ wastewater gc/l and $x_r$ the rain measurement for\n",
    "that date, $a,b,c$ are fitted coefficients.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "east_pm = cases_water_pm[cases_water_pm['Region name'] == 'Eastern'].copy()\n",
    "east_pm['DateNum'] = mdates.date2num(pd.to_datetime(east_pm['Date']))\n",
    "\n",
    "X_nu = east_pm['DateNum'] \n",
    "y_nu = east_pm['newCasesBySpecimenDate']\n",
    "X_approx = np.linspace(X_nu.min(), X_nu.max(), num=1000)\n",
    "y1 = east_pm['newCasesBySpecimenDate'][1:-1]\n",
    "\n",
    "y2 = east_pm['gc/l'][1:-1]\n",
    "x2 = east_pm['DateNum'][1:-1]\n",
    "x3 = east_pm['Rainfall'][1:-1]\n",
    "\n",
    "y2_f = east_pm['gc/l']\n",
    "x2_f = east_pm['DateNum']\n",
    "x3_f = east_pm['Rainfall']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_f3(X,m, d, w):\n",
    "    return m*np.exp(d*X[0]) * (X[1]*X[2]*w)\n",
    "\n",
    "X = np.vstack((x2,y2,x3))\n",
    "popt, pcov = curve_fit(exp_f3, X, y1, p0=[start, start,1], maxfev=100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_approx = np.dstack((np.linspace(X_nu.min(), X_nu.max(), num=1000),\n",
    "                      np.linspace(X[1].min(), X[1].max(), num=1000), \n",
    "                      np.linspace(X[2].min(), X[2].max(), num=1000)))[0].T\n",
    "y_hat = exp_f3(X_approx, *popt)\n",
    "\n",
    "fig = go.Figure(data=go.Scatter(x=mdates.num2date(X_nu), y=y_nu, name='Number of new positive cases'))\n",
    "fig.add_trace(go.Scatter(x=mdates.num2date(X_approx[0]), y=y_hat, name='Exponential Curve Fit'))\n",
    "fig.add_trace(go.Scatter(x=mdates.num2date(x1), y=y1, name='Training values'))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_3 = np.vstack((x2_f,y2_f,x3_f))\n",
    "calc_r2(X_3, popt, y_nu, exp_f3)\n",
    "print(f'R^2: {calc_r2(X_3, popt, y_nu, exp_f3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion & Discussion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### The simple exponential model using rainfall and gc/l to estimate case numbers is unsuitable\n",
    "\n",
    "From the previous model and figure we can see that the model using gc/l and\n",
    "rainfall is not terrible, but it isn't great either. Perhaps most encouraging is\n",
    "that from the models prior we determined that gc/l could not provide any useful\n",
    "results when used alone, the inclusion of rainwater giving some increase in\n",
    "accuracy could be encouraging (it could also be something quite obscure\n",
    "happening also!). \n",
    "\n",
    "From our own results here we cannot discount the use of wastewater data and\n",
    "weather data for use in predicting disease case numbers, but it is clear that\n",
    "the models we have chosen need refined or perhaps completely rethought. \n",
    "\n",
    "\n",
    "### Looking at these models in different points in time and space\n",
    "\n",
    "As discussed in the beginning we average firstly by time (monthly) and then\n",
    "again spatially (by England regions. We then focused a lot of the models on a\n",
    "specific region of England. The logical future work, which shouldn't be too\n",
    "difficult would be to explore these models, with minimal change, on the other\n",
    "regional data we currently have. It would be interesting to see if any areas are\n",
    "better/worse performing for the model. \n",
    "\n",
    "Additionally, the data we have decided to use is very constrained in terms of\n",
    "time, a month is a significant amount of time for something which grows\n",
    "exponentially like disease case numbers. Slightly better data wrangling could\n",
    "easily allow us to repeat all analysis with roughly ~4x the data points if we\n",
    "consider the problem weekly rather than monthly. Once again, this better\n",
    "resolution may change our outlook/result from the current models. \n",
    "\n",
    "### Wastewater gc/l could be used to monitor disease trends\n",
    "\n",
    "As we observed when initially compiling and organising the data, there is a\n",
    "rough agreement, albeit in shape not magnitude, of case numbers and wastewater\n",
    "gc/l. This could suggest that these data may be suitable as “warning” systems\n",
    "rather than predictors of case numbers. \n",
    "\n",
    "Therefore, these data could be used to target areas which may need\n",
    "interventions, such as increased disease testing, tracking/tracing or other\n",
    "appropriate responses. \n",
    "\n",
    "### The ethics of using wastewater data\n",
    "\n",
    "One of the major strengths, and desire for these data to be used, is that they\n",
    "are relatively unbiased. Everyone who lives in an area contributes to wastewater\n",
    "and therefore doesn't have the issue of ill people being more prone to being\n",
    "tested and recorded. However, some concerns have been raised over the ethics of\n",
    "this level of monitoring of people without explicit consent. [Gableetal.](https://academic.oup.com/jlb/article/7/1/lsaa039/5861905),\n",
    "for example, suggest that these data sources be treated similarly to direct\n",
    "testing methods (e.g. PCR/LFT). One could make the argument that the privacy of\n",
    "individuals is at risk when performing these analysis. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "bfb6be6a1e2928ff2d2ab7904db3c9037c4b5ca4f399bd5e7064365a54a3f091"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
